{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d524ad8f-d024-4ba3-8bbc-2435ec3d0dba",
    "_uuid": "7bb30a6f2a18be45a491092a6e2dfcdcce71a2a0"
   },
   "source": [
    "## GA Capstone Project\n",
    "# TensorFlow Speech Recognition Challenge\n",
    "This notebook builds a light-weight CNN to address the TensorFlow Speech Recognition Challenge. BingQuing Wei and DavidS provided the audio to spectrogram insight along with the initial Keras CNN insight.\n",
    "\n",
    "The original training set wave files are sampled at 16000. To cut down on processing time they are resampled at a rate of 8000.\n",
    "\n",
    "Current best LB is 0.73. Epoch set to 10. (Sample rate halfed to 8000 from 16000)\n",
    "\n",
    "Link: https://www.kaggle.com/c/tensorflow-speech-recognition-challenge\n",
    "\n",
    "\n",
    "\n",
    "## File Structure\n",
    "The current file strucuture for running this notebook:\n",
    "\n",
    "speechCap\n",
    "\n",
    "├── test            \n",
    "\n",
    "│   └── audio #test wavfiles\n",
    "\n",
    "├── train           \n",
    "\n",
    "│   ├── audio #train wavfiles\n",
    "\n",
    "└── model #store models\n",
    "\n",
    "│\n",
    "\n",
    "└── out #store sub.csv\n",
    "\n",
    "## Techniques to improve the current score\n",
    "This is a light model for quick scoring. Improvements will come by:\n",
    "\n",
    "1. Use original wav files instead resampled ones.\n",
    "2. Create more 'silence' wav files using chop_audio.\n",
    "3. Build deeper CNN or use RNN.\n",
    "4. Train for longer epochs\n",
    "5. Use Transfer Learning - take a NN that has been trained on a similar data set, and retrain the last few layers of the network for new categories.\n",
    "6. Vary the hyperparameters:\n",
    " - Train for additional epochs\n",
    " - Try dropout rates btw 0.2 - 0.35\n",
    " - Try SGD and other optimization functions\n",
    " - Vary the learning rate (lower learning rate = reliable but long training. Higher learning rate = training may not converge or may overshoot but training will be faster.)\n",
    "\n",
    "## TensorFlow tutorial \n",
    "The baseline for the TensorFlow tutorial is 0.88.\n",
    "Link: https://www.tensorflow.org/versions/master/tutorials/audio_recognition\n",
    "\n",
    "\n",
    "## Appendix\n",
    "__BingQing Wei__ provided excellent insight with his notebook on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "f7fd8bcb-4451-4d47-bfe8-491c94b3b4eb",
    "_uuid": "712710f20b00f97271136cfeab9937a4c6a2458b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from glob import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "from keras import optimizers, losses, activations, models\n",
    "# Core layers and Convolutional layers that help train on image data\n",
    "from keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fb35a2f1-9301-4693-a9ef-9d180b630f05",
    "_uuid": "4b1ba61998e14e15c822c605dbe5961bfed36014"
   },
   "source": [
    "The original sample rate is 16000, and we will resample it to 8000 to reduce data size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "dc66e1df-f1eb-4df4-ba1a-65b9f1675953",
    "_uuid": "4cc586519523b28d1d595716d8709ace9f27ac9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\train\\audio\n",
      ".\\test\\audio\n"
     ]
    }
   ],
   "source": [
    "L = 16000\n",
    "legal_labels = 'yes no up down left right on off stop go silence unknown'.split()\n",
    "\n",
    "#src folders`\n",
    "#root_path = r'..'\n",
    "root_path = r'.'\n",
    "out_path = r'./out'\n",
    "model_path = r'./model'\n",
    "train_data_path = os.path.join(root_path, 'train', 'audio')\n",
    "test_data_path = os.path.join(root_path,  'test', 'audio')\n",
    "\n",
    "print(train_data_path)    #debug\n",
    "print(test_data_path)     #debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA - sample spectrogram\n",
    "- one example for 'yes' audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ea77908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename = '/yes/0a7c2a8d_nohash_0.wav'\n",
    "sample_rate, samples = wavfile.read(str(train_data_path) + filename)\n",
    "frequencies, times, spectogram = signal.spectrogram(samples, sample_rate)\n",
    "\n",
    "\n",
    "plt.pcolormesh(times, frequencies, spectogram)\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e53561e4-1c98-44c0-9245-d87f7957faa5",
    "_uuid": "d9a08781f22e574bb1eb0dc29adeb8dddebc8b51"
   },
   "source": [
    "custom_fft and log_specgram functions. These were written by __DavidS__.\n",
    "Taking the audio files and converting them to images for analysis by CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "0fd0b579-8b6f-4253-bf3a-7f75115a42d6",
    "_uuid": "e7ea2c277b6459e532721452ec3cd80d585eae1e"
   },
   "outputs": [],
   "source": [
    "def custom_fft(y, fs):\n",
    "    T = 1.0 / fs\n",
    "    N = y.shape[0]\n",
    "    yf = fft(y)\n",
    "    xf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
    "    # FFT is simmetrical, so we take just the first half\n",
    "    # FFT is also complex, to we take just the real part (abs)\n",
    "    vals = 2.0/N * np.abs(yf[0:N//2])\n",
    "    return xf, vals\n",
    "\n",
    "def log_specgram(audio, sample_rate, window_size=20,\n",
    "                 step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c54cda36-777e-4129-bac1-af2d1ed2706e",
    "_uuid": "5a04e71fe7e66e1a31835feebdfef4c63920faf8"
   },
   "source": [
    "Following is the utility function to grab all wav files inside train data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my test of this function\n",
    "\n",
    "def list_wavs_fname(dirpath, ext='wav'):\n",
    "    print(dirpath)\n",
    "    fpaths = glob(os.path.join(dirpath, r'*/*' + ext))\n",
    "   # pat = r'.+/(\\w+)/\\w+\\.' + ext + '$'\n",
    "    pat = r'(\\w{1,})'\n",
    "    print(pat)\n",
    "    labels = []\n",
    "    #print(fpaths)        #debug\n",
    "    for fpath in fpaths:\n",
    "        r = re.findall(pat, fpath)\n",
    "        #print(r)     #debug ==>> this is always 'NONE'..so no labels and no fnames\n",
    "        if r:\n",
    "            labels.append(r[2])      # append the label\n",
    "            \n",
    "    #pat = r'.+/(\\w+\\.' + ext + ')$'\n",
    "    fnames = []\n",
    "    for fpath in fpaths:\n",
    "        #r = re.match(pat, fpath)\n",
    "        r = re.findall(pat, fpath)\n",
    "        if r:\n",
    "            newfile = r[3] + '.' + r[4]\n",
    "            #fnames.append(r[3])\n",
    "            fnames.append(newfile)\n",
    "    return labels, fnames\n",
    "\n",
    "#test\n",
    "#labels, fnames = list_wavs_fname(train_data_path)      #debug\n",
    "#print('==>Labels <==')\n",
    "#print(labels)     #debug\n",
    "#print('==>File names <==')\n",
    "#print(fnames)     #debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "41025a55-8497-43cf-b316-003af7d9d19f",
    "_uuid": "fc18e87793888952e81a867dd95b1dcc455f9932"
   },
   "source": [
    "__pad_audio__ will pad audios that are less than 16000(1 second) with 0s to make them all have the same length.\n",
    "\n",
    "__chop_audio__ will chop audios that are larger than 16000(eg. wav files in background noises folder) to 16000 in length. In addition, it will create several chunks out of one large wav files given the parameter 'num'.\n",
    "\n",
    "__label_transform__ transform labels into dummies values. It's used in combination with softmax to predict the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "200c34a1-851a-4447-9ff7-b4e541f090c6",
    "_uuid": "94e40aef3899acfd3ed85557caa66fee5dd47db2"
   },
   "outputs": [],
   "source": [
    "def pad_audio(samples):\n",
    "    if len(samples) >= L: return samples\n",
    "    else: return np.pad(samples, pad_width=(L - len(samples), 0), mode='constant', constant_values=(0, 0))\n",
    "\n",
    "def chop_audio(samples, L=16000, num=20):\n",
    "    for i in range(num):\n",
    "        beg = np.random.randint(0, len(samples) - L)\n",
    "        yield samples[beg: beg + L]\n",
    "\n",
    "def label_transform(labels):\n",
    "    nlabels = []\n",
    "    for label in labels:\n",
    "        if label == '_background_noise_':\n",
    "            nlabels.append('silence')\n",
    "        elif label not in legal_labels:\n",
    "            nlabels.append('unknown')\n",
    "        else:\n",
    "            nlabels.append(label)\n",
    "    return pd.get_dummies(pd.Series(nlabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dae2a45a-f7ab-4e84-bc73-688eda6eca8e",
    "_uuid": "267314ef41c459c8b6ab903d721980fdd62b4106"
   },
   "source": [
    "Next, we use functions declared above to generate x_train and y_train.\n",
    "label_index is the index used by pandas to create dummy values, we need to save it for later use.\n",
    "i.e. reads in the .wav files and processes them in to spectograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "4c8d9fdf-ea3e-45fa-b7ef-52542c70b9db",
    "_uuid": "81bc9722dfb036c73721ae44829d429489662e75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\train\\audio\n",
      "(\\w{1,})\n",
      ".\\train\\audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IBM_ADMIN\\Miniconda3\\envs\\GA\\lib\\site-packages\\scipy\\io\\wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, fnames = list_wavs_fname(train_data_path)\n",
    "print(train_data_path)     #debug\n",
    "#print(labels)      #debug\n",
    "#print(fnames)      #debug\n",
    "\n",
    "new_sample_rate = 8000\n",
    "#new_sample_rate = 16000     # keep original sample rate\n",
    "y_train = []\n",
    "x_train = []\n",
    "\n",
    "for label, fname in zip(labels, fnames):\n",
    "    #print(\"=> train path, label, file name\") #debug\n",
    "    #print(train_data_path, label, fname)    #debug\n",
    "    sample_rate, samples = wavfile.read(os.path.join(train_data_path, label, fname))\n",
    "    #print(sample_rate, samples)     #debug\n",
    "    samples = pad_audio(samples)\n",
    "    if len(samples) > 16000:\n",
    "        n_samples = chop_audio(samples)\n",
    "    else: n_samples = [samples]\n",
    "    for samples in n_samples:\n",
    "        resampled = signal.resample(samples, int(new_sample_rate / sample_rate * samples.shape[0]))\n",
    "        _, _, specgram = log_specgram(resampled, sample_rate=new_sample_rate)\n",
    "        y_train.append(label)\n",
    "        x_train.append(specgram)\n",
    "x_train = np.array(x_train)\n",
    "x_train = x_train.reshape(tuple(list(x_train.shape) + [1]))\n",
    "y_train = label_transform(y_train)\n",
    "label_index = y_train.columns.values\n",
    "y_train = y_train.values\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "del labels, fnames\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58356, 99, 161, 1)\n",
      "(58356, 12)\n"
     ]
    }
   ],
   "source": [
    "# my debug box\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)   # 64,841 samples... 12 dimensional matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "56921cf3-1269-4b29-876d-abdd31eb150a",
    "_uuid": "a87a77b76c42da61ca0bec395c71bef795a9e928"
   },
   "source": [
    "CNN declared below.\n",
    "The specgram created will be of shape (99, 81), but in order to fit into Conv2D layer, we need to reshape it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b97e8887-b593-4d88-95c8-fc8f1dd5ca72",
    "_uuid": "60af394ad8e91fb868ea32dbb6ac6a725b5935c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 99, 161, 1)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 99, 161, 1)        4         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 98, 160, 8)        40        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 97, 159, 8)        264       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 48, 79, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 48, 79, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 46, 77, 16)        1168      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 44, 75, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 22, 37, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 22, 37, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 20, 35, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 17, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10, 17, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5440)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               696448    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                1548      \n",
      "=================================================================\n",
      "Total params: 723,968\n",
      "Trainable params: 723,454\n",
      "Non-trainable params: 514\n",
      "_________________________________________________________________\n",
      "Train on 58356 samples, validate on 6485 samples\n",
      "Epoch 1/10\n",
      " - 591s - loss: 1.0806 - acc: 0.6862 - val_loss: 0.5846 - val_acc: 0.8122\n",
      "Epoch 2/10\n",
      " - 606s - loss: 0.5370 - acc: 0.8237 - val_loss: 0.3786 - val_acc: 0.8790\n",
      "Epoch 3/10\n",
      " - 592s - loss: 0.4166 - acc: 0.8643 - val_loss: 0.2889 - val_acc: 0.9115\n",
      "Epoch 4/10\n",
      " - 593s - loss: 0.3542 - acc: 0.8840 - val_loss: 0.2357 - val_acc: 0.9243\n",
      "Epoch 5/10\n",
      " - 592s - loss: 0.3101 - acc: 0.8989 - val_loss: 0.2555 - val_acc: 0.9192\n",
      "Epoch 6/10\n",
      " - 591s - loss: 0.2814 - acc: 0.9079 - val_loss: 0.2151 - val_acc: 0.9291\n",
      "Epoch 7/10\n",
      " - 591s - loss: 0.2592 - acc: 0.9154 - val_loss: 0.1932 - val_acc: 0.9399\n",
      "Epoch 8/10\n",
      " - 592s - loss: 0.2410 - acc: 0.9226 - val_loss: 0.1830 - val_acc: 0.9443\n",
      "Epoch 9/10\n",
      " - 592s - loss: 0.2272 - acc: 0.9255 - val_loss: 0.1818 - val_acc: 0.9403\n",
      "Epoch 10/10\n",
      " - 592s - loss: 0.2121 - acc: 0.9307 - val_loss: 0.1642 - val_acc: 0.9503\n"
     ]
    }
   ],
   "source": [
    "#input_shape = (99, 81, 1)             # 3D tensor\n",
    "input_shape = (99, 161, 1)             # for 16,000 sample rate\n",
    "nclass = 12                           # number of labels\n",
    "inp = Input(shape=input_shape)\n",
    "norm_inp = BatchNormalization()(inp)\n",
    "# 8 = num of convolution filters, 2 = num of rows (& columns)for each convo kernel\n",
    "# step size is (1,1) by default. 'subsample' to change it.\n",
    "# n'relu' activation is performant cheap. NN runs faster.\n",
    "img_1 = Convolution2D(8, kernel_size=2, activation=activations.relu)(norm_inp)\n",
    "img_1 = Convolution2D(8, kernel_size=2, activation=activations.relu)(img_1)\n",
    "# Reduce the num of parameters in the model\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)           # vertical, horizontal dimensions\n",
    "# To reduce over-fitting\n",
    "img_1 = Dropout(rate=0.2)(img_1)                        # fraction of units to drop. increase for more dropout\n",
    "img_1 = Convolution2D(16, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = Convolution2D(16, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Convolution2D(32, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Flatten()(img_1)                            # Flatten the weights before passing to Dense output layer\n",
    "\n",
    "# First param (128) is the output size of the layer\n",
    "dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(img_1))\n",
    "dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(dense_1))\n",
    "# Final layer has output of 12 - same as our label count\n",
    "dense_1 = Dense(nclass, activation=activations.softmax)(dense_1)\n",
    "\n",
    "# Two args required to comile model\n",
    "model = models.Model(inputs=inp, outputs=dense_1)\n",
    "opt = optimizers.Adam()\n",
    "\n",
    "# 1st try\n",
    "#model.compile(optimizer=opt, loss=losses.binary_crossentropy)\n",
    "#model.compile(optimizer=opt, loss=losses.binary_crossentropy, metrics=[]'accuracy'])\n",
    "# 2nd try - change to categorical_crossentropy\n",
    "model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state=2017)\n",
    "model.fit(x_train, y_train, batch_size=16, validation_data=(x_valid, y_valid), epochs=10, shuffle=True, verbose=2)\n",
    "\n",
    "model.save(os.path.join(model_path, 'cnnCategoricalEpoch10Drop02S16.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "060811f5-34ac-4fc2-92ba-c4276606c2a0",
    "_uuid": "2e3fa8d9706f47e69d0b74afcb68280f8a5de706"
   },
   "source": [
    "Test data is way too large to fit in RAM, we need to process them one by one.\n",
    "Generator test_data_generator will create batches of test wav files to feed into CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "7dfe0801-a636-4123-8367-ab2f19c97800",
    "_uuid": "646b6bcfbde7eae53cd8822b8838c575859e51ce"
   },
   "outputs": [],
   "source": [
    "def test_data_generator(batch=16):\n",
    "    fpaths = glob(os.path.join(test_data_path, '*wav'))\n",
    "    i = 0\n",
    "    for path in fpaths:\n",
    "        if i == 0:\n",
    "            imgs = []\n",
    "            fnames = []\n",
    "        i += 1\n",
    "        rate, samples = wavfile.read(path)\n",
    "        samples = pad_audio(samples)\n",
    "        resampled = signal.resample(samples, int(new_sample_rate / rate * samples.shape[0]))\n",
    "        _, _, specgram = log_specgram(resampled, sample_rate=new_sample_rate)\n",
    "        imgs.append(specgram)\n",
    "        fnames.append(path.split('\\\\')[-1])\n",
    "        if i == batch:\n",
    "            i = 0\n",
    "            imgs = np.array(imgs)\n",
    "            imgs = imgs.reshape(tuple(list(imgs.shape) + [1]))\n",
    "            yield fnames, imgs\n",
    "    if i < batch:\n",
    "        imgs = np.array(imgs)\n",
    "        imgs = imgs.reshape(tuple(list(imgs.shape) + [1]))\n",
    "        yield fnames, imgs\n",
    "    raise StopIteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "22992a27-deda-4a35-b34c-4aa87ad173ec",
    "_uuid": "c6d2516a6d5bd3c6a108d7e28565edaa65830958"
   },
   "source": [
    "We use the trained model to predict the test data's labels.\n",
    "However, since Kaggle doesn't provide test data, the following sections won't be executed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "7fa8feb8-236e-46c5-8432-014f7e27484d",
    "_uuid": "56194039cac16f5d86a322e67641cbeafda9857d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IBM_ADMIN\\Miniconda3\\envs\\GA\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: generator 'test_data_generator' raised StopIteration\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "#exit() #delete this\n",
    "#del x_train, y_train\n",
    "#gc.collect()\n",
    "\n",
    "index = []\n",
    "results = []\n",
    "for fnames, imgs in test_data_generator(batch=32):\n",
    "    predicts = model.predict(imgs)\n",
    "    predicts = np.argmax(predicts, axis=1)\n",
    "    predicts = [label_index[p] for p in predicts]\n",
    "    index.extend(fnames)\n",
    "    results.extend(predicts)\n",
    "\n",
    "df = pd.DataFrame(columns=['fname', 'label'])\n",
    "df['fname'] = index\n",
    "df['label'] = results\n",
    "df.to_csv(os.path.join(out_path, 'subCEepoch10Drop02.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
